{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Intensity Prediction - ANGER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from pprint import pprint\n",
    "from scipy import stats\n",
    "from numpy import sqrt\n",
    "from time import time\n",
    "import pandas, re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables for files, lists & dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started the stopwatch...\n"
     ]
    }
   ],
   "source": [
    "## global vars for file\n",
    "TRAIN_DATA_PATH = 'anger_train.csv'\n",
    "TEST_DATA_PATH = 'anger_test.csv'\n",
    "HASHTAG_EMOTION_LEXICON = 'anger_hashtag.csv'\n",
    "WORD_EMOTION_LEXICON = 'anger_emotion.csv'\n",
    "EMOTION_EXPANDED = 'emotion_expanded.csv'\n",
    "\n",
    "\n",
    "print(\"Started the stopwatch...\")\n",
    "start_time = time()\n",
    "\n",
    "# global vars\n",
    "X_Train = []            # training set with features\n",
    "Y_Train = []            # list of all emotion value results from training set\n",
    "Train_Sentence = []     # list of all sentences from anger training set\n",
    "\n",
    "X_Test = []             # testing set with features\n",
    "Y_Test = []             # list of all emotion value results from testing set\n",
    "Test_Sentence = []      # list of all sentences from anger testing set\n",
    "\n",
    "SIA_Vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "anger_hashtag = {}        # dict with keys as hashtag words & values as anger emotions\n",
    "word_emotion = {}       # dict with keys as words that represent anger, values = boolean True\n",
    "emotion_expanded = {}   # dict with keys as words & values with thier anger rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load required files\n",
    "**Syntax -**\n",
    "+ File name in lexicon = FILE_NAME_ALIAS : INFO\n",
    "<br>\n",
    "1. **'5. NRC-Hashtag-Emotion-Lexicon-v0.2.txt'** = HASHTAG_EMOTION_LEXICON : Contains hashtag words with their respective anger values\n",
    "2. **'8. NRC-word-emotion-lexicon.txt'** = WORD_EMOTION_LEXICON : Contains list of emotion words which represent anger\n",
    "3. **'6. NRC-10-expanded.csv'** = EMOTION_EXPANDED : Contains various emotion intensity for anger words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load files -> 0.7710826396942139\n"
     ]
    }
   ],
   "source": [
    "#### load files\n",
    "\n",
    "with open(HASHTAG_EMOTION_LEXICON, 'r') as f:\n",
    "    for i in f:\n",
    "        line = list(map(str,i.split()))\n",
    "        anger_hashtag[line[1]] = float(line[2])\n",
    "\n",
    "with open(WORD_EMOTION_LEXICON, 'r') as f:\n",
    "    for i in f:\n",
    "        line = list(map(str,i.split()))\n",
    "        word_emotion[line[0]] = True\n",
    "\n",
    "exp_emo = pandas.read_csv(EMOTION_EXPANDED, sep = \"\\t\")\n",
    "for i in range(len(exp_emo)):\n",
    "    emotion_expanded[exp_emo['word'][i]] = float(exp_emo[\"joy\"][i])     # dict = 'word' : float(anger value)\n",
    "\n",
    "print(\"Time to load files ->\",time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature functions\n",
    "1. check_elongation : Counts the freq of elongated words in a tweet\n",
    "2. check_hashtag : Counts the freq of hashtags used in a tweet\n",
    "3. check_CAPS : Counts the freq of CAPS words in a tweet\n",
    "4. check_tag : Counts the freq of tagged people\n",
    "5. check_negation : Counts the freq of negative words in a tweet\n",
    "6. check_word_emotion : Count the freq of words found in **word_emotion** in a tweet\n",
    "7. check_anger_hashtag : Return the average score of words found in **anger_hashtag** in a tweet\n",
    "8. check_exp_emo : Return the average score of words found in **emotion_expanded** in a tweet\n",
    "9. VADER : Append the list of dict values {'pos', 'neu', 'neg', 'compound'} as given by polarity_score\n",
    "\n",
    "* More features ahead like punctuations, Count Vectorization (unigram + bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### features #################\n",
    "\n",
    "def check_elongation(word):\n",
    "    temp = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    if len(temp) != len(word):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def check_hashtag(word):\n",
    "    if word[0] != \"#\":\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def check_CAPS(word):\n",
    "    if word.isupper():\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def check_tag(word):\n",
    "    if word[0] != \"@\":\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def check_negation(word):\n",
    "    if word.lower() in ['not', 'no', 'nope', 'nopes', 'never', 'neither', 'nor', 'none']:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def check_word_emotion(word):\n",
    "    if word.lower() not in word_emotion:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def check_anger_hashtag(line):\n",
    "    score = []\n",
    "    for word in line:\n",
    "        if word.lower() in anger_hashtag:\n",
    "            score.append(anger_hashtag[word.lower()])\n",
    "    if score != []:\n",
    "        return sum(score)/len(score)\n",
    "    return 0\n",
    "\n",
    "def check_exp_emo(line):\n",
    "    score = []\n",
    "    for word in line:\n",
    "        if word.lower() in emotion_expanded:\n",
    "            score.append(emotion_expanded[word.lower()])\n",
    "    if score != []:\n",
    "        return sum(score)/len(score)\n",
    "    return 0    \n",
    "\n",
    "def VADER(X_T, sentences):\n",
    "    count = 0\n",
    "    for i in sentences:\n",
    "        score = SIA_Vader.polarity_scores(i)\n",
    "        X_T[count].append(score['pos'])\n",
    "        X_T[count].append(score['neu'])\n",
    "        X_T[count].append(score['neg'])\n",
    "        X_T[count].append(score['compound'])\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess function\n",
    "* Function to make dataset for **X_Train** , **Y_Train** , **X_Test** , **Y_Test**\n",
    "* Adding all the features\n",
    "* Make list of Training & Testing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ preprocess function ########################\n",
    " \n",
    "def TT_preprocess(dataset, sentences, Y_T, X_T):    # preprocess Train & Test data\n",
    "    for i in dataset:\n",
    "        X_T.append([0,0,0,0,0,0,0,0,])              # append feaures in X_Train / X_Test\n",
    "        \n",
    "        line = list(map(str,i.split()))\n",
    "        Y_T.append(float(line[-1]))\n",
    "        \n",
    "        for word in line[1:-2]:\n",
    "            X_T[-1][0] += check_elongation(word)\n",
    "            X_T[-1][1] += check_hashtag(word)\n",
    "            X_T[-1][2] += check_CAPS(word)\n",
    "            X_T[-1][3] += check_tag(word)\n",
    "            X_T[-1][4] += check_negation(word)\n",
    "            X_T[-1][5] += check_word_emotion(word)\n",
    "\n",
    "        X_T[-1][6] +=  check_anger_hashtag(line)\n",
    "        X_T[-1][7] +=  check_exp_emo(line)\n",
    "        sentences.append(' '.join(line[1:-2]))\n",
    "    \n",
    "    VADER(X_T, sentences)   # appends X_T[-1][8:13], 4 new columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training & Testing datasets\n",
    "* Load training & testing data\n",
    "* Call preprocess function on them\n",
    "* Make X_Train , Y_Train , X_Test & Y_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to preprocess training & testing data -> 1.4066758155822754\n"
     ]
    }
   ],
   "source": [
    "########## load & preprocess files\n",
    "\n",
    "with open(TRAIN_DATA_PATH, 'r') as f:\n",
    "    TT_preprocess(f, Train_Sentence, Y_Train, X_Train)\n",
    "\n",
    "with open(TEST_DATA_PATH, 'r') as f:\n",
    "    TT_preprocess(f, Test_Sentence, Y_Test, X_Test)\n",
    "\n",
    "\n",
    "print(\"Time to preprocess training & testing data ->\",time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorization\n",
    "* Convert all X/Y_Train & X/Y_Test to DataFrames\n",
    "* Apply Count Vectorization on all Training & Testing sentences (unigram + bigram)\n",
    "* Convert it to pandas dataframe & concatenate it with X_Train & X_Test respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making DF...\n",
      "Time to apply count Vectorization, make DFs & merge them -> 1.564823865890503\n"
     ]
    }
   ],
   "source": [
    "####################### convert to dataframes\n",
    "\n",
    "print(\"Making DF...\")\n",
    "X_Train_DF = pandas.DataFrame(X_Train)\n",
    "Y_Train_DF = pandas.DataFrame(Y_Train)\n",
    "X_Test_DF = pandas.DataFrame(X_Test)\n",
    "Y_Test_DF = pandas.DataFrame(Y_Test)\n",
    "\n",
    "####################### apply count vectorization\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))    # Unigram and Bigram\n",
    "Vectorized_Train = count_vectorizer.fit_transform(Train_Sentence)\n",
    "Vectorized_Test = count_vectorizer.transform(Test_Sentence)\n",
    "\n",
    "########### conver to DF & concat to X_Train & X_Test\n",
    "\n",
    "# Convert sparse csr_matrix to dense format and allow columns to contain the array mapping from feature integer indices to feature names\n",
    "count_vect_df = pandas.DataFrame(Vectorized_Train.todense(), columns=count_vectorizer.get_feature_names())\n",
    "# Concatenate the original X_Train and the count_vect_df columnwise.\n",
    "X_Train_DF = pandas.concat([X_Train_DF, count_vect_df], axis=1)\n",
    "\n",
    "# Convert sparse csr_matrix to dense format and allow columns to contain the array mapping from feature integer indices to feature names\n",
    "count_vect_df = pandas.DataFrame(Vectorized_Test.todense(), columns=count_vectorizer.get_feature_names())\n",
    "# Concatenate the original X_Train and the count_vect_df columnwise.\n",
    "X_Test_DF = pandas.concat([X_Test_DF, count_vect_df], axis=1)\n",
    "\n",
    "\n",
    "print(\"Time to apply count Vectorization, make DFs & merge them ->\",time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics function\n",
    "* Function that takes model predicted results & actual results\n",
    "* Compares them & print - <br>\n",
    "`Mean Absolute Error` <br>\n",
    "`Mean Squared Error` <br>\n",
    "`Root Mean Squared Error` <br>\n",
    "`R2 - Score` <br>\n",
    "`Pearson correlation, p-value` <br>\n",
    "`Spearman Result`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(result, Y_Test):\n",
    "    MAE = metrics.mean_absolute_error(Y_Test, result)    \n",
    "    MSE = metrics.mean_squared_error(Y_Test, result)     \n",
    "    rmse = sqrt(MSE)\n",
    "    r2 = metrics.r2_score(Y_Test, result)\n",
    "\n",
    "    print(\"Results of sklearn.metrics:\")\n",
    "    print(\"MAE:\",MAE)\n",
    "    print(\"MSE:\", MSE)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    print(\"R-Squared:\", r2)\n",
    "    print(\"\\npearson corr. , p valve =\",stats.pearsonr(Y_Test,result))\n",
    "    print(stats.spearmanr(Y_Test,result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_prediction function\n",
    "* Takes input as model classifier & the DataFrames\n",
    "* Prepares model & predict results\n",
    "* Calls get_statistics function for final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(classifier, X_Train_DF, Y_Train, X_Test_DF, Y_Test):\n",
    "    model = classifier\n",
    "    model.fit(X_Train_DF, Y_Train)\n",
    "    result = model.predict(X_Test_DF) \n",
    "    \n",
    "    get_statistics(result, Y_Test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result for SVM ->\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.11319175362244595\n",
      "MSE: 0.019879849385711378\n",
      "RMSE: 0.14099591974845008\n",
      "R-Squared: 0.3262405365895137\n",
      "\n",
      "pearson corr. , p valve = (0.5790818107120931, 2.9132508996171585e-69)\n",
      "SpearmanrResult(correlation=0.5689982163886088, pvalue=2.06129836924723e-66)\n",
      "Time taken by SVM model -> 11.63190221786499\n"
     ]
    }
   ],
   "source": [
    "## SVM\n",
    "print(\"\\nResult for SVM ->\")\n",
    "model_prediction(SVR(), X_Train_DF, Y_Train, X_Test_DF, Y_Test)\n",
    "print(\"Time taken by SVM model ->\",time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result Decision Tree ->\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.1154059323323111\n",
      "MSE: 0.020980108851885355\n",
      "RMSE: 0.14484512022117058\n",
      "R-Squared: 0.2889510072194118\n",
      "\n",
      "pearson corr. , p valve = (0.547594469013182, 1.1151218338857375e-60)\n",
      "SpearmanrResult(correlation=0.5507191215289514, pvalue=1.7222153245789713e-61)\n",
      "Time taken by Decision Tree -> 11.959733486175537\n"
     ]
    }
   ],
   "source": [
    "## Using Decision Tree\n",
    "print(\"\\nResult Decision Tree ->\")\n",
    "model_prediction(DecisionTreeRegressor(max_depth = 5), X_Train_DF, Y_Train, X_Test_DF, Y_Test)\n",
    "print(\"Time taken by Decision Tree ->\",time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result for MLP ->\n",
      "Results of sklearn.metrics:\n",
      "MAE: 0.11389595058964681\n",
      "MSE: 0.020303883056515853\n",
      "RMSE: 0.14249169469311485\n",
      "R-Squared: 0.311869366417845\n",
      "\n",
      "pearson corr. , p valve = (0.5831974578708614, 1.872611882396523e-70)\n",
      "SpearmanrResult(correlation=0.5732742289854856, pvalue=1.3114177982316639e-67)\n",
      "Time taken by MLP -> 18.60454535484314\n"
     ]
    }
   ],
   "source": [
    "## using MLP\n",
    "print(\"\\nResult for MLP ->\")\n",
    "clf = MLPRegressor(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1,max_iter=1000)\n",
    "model_prediction(clf, X_Train_DF, Y_Train, X_Test_DF, Y_Test)\n",
    "print(\"Time taken by MLP ->\",time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
